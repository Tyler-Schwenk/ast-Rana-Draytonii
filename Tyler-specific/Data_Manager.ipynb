{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QIgXlOX0IN4"
      },
      "source": [
        "This next cell will split given audio files of any length into 10 second segments, and save them to the specified output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzD23XKRjudJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "def split_all_audio_files(input_dir, output_dir, segment_length_sec=10):\n",
        "    # Make sure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loop over all files in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        # Check if the file is a .wav file\n",
        "        if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "            try:\n",
        "                # Full path to the original audio file\n",
        "                audio_path = os.path.join(input_dir, filename)\n",
        "                # Load the audio file\n",
        "                waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "                # Calculate number of samples in segment_length_sec\n",
        "                num_samples_segment = segment_length_sec * sample_rate\n",
        "\n",
        "                # Calculate total number of segments\n",
        "                total_segments = math.ceil(waveform.shape[1] / num_samples_segment)\n",
        "\n",
        "                # Split waveform into segments and save each segment to a new .wav file\n",
        "                for i in range(total_segments):\n",
        "                    start = i * num_samples_segment\n",
        "                    end = start + num_samples_segment\n",
        "                    segment = waveform[:, start:end]\n",
        "                    \n",
        "                    # Prepare filename for the segment\n",
        "                    segment_filename = f\"{filename.rstrip('.wav')}_segment{i}.wav\"\n",
        "                    segment_path = os.path.join(output_dir, segment_filename)\n",
        "\n",
        "                    # Save segment as a .wav file\n",
        "                    segment = (segment * 32767).short()  # Convert to 16-bit PCM format\n",
        "                    torchaudio.save(segment_path, segment, sample_rate)\n",
        "            except Exception as e:\n",
        "                  print(f\"Error processing file {audio_path}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHy6iYlR0klJ"
      },
      "outputs": [],
      "source": [
        "input_dir = '/content/NegativeSamples'\n",
        "output_dir = '/content/SplitNegative'\n",
        "split_all_audio_files(input_dir, output_dir, segment_length_sec=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boJgdt6BXYuy"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "zip and download split files to local. or use google drive\n",
        "\n",
        "files stored on google colab do not persist. I will download these files to review the positives again as some segments will no longer have our call after splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6xBv1zHXw-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab46525f-8bc9-4478-c656-5d4304778922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/SplitNegative\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/SplitNegative.zip . -i /content/SplitNegative)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/SplitNegative.zip /content/SplitNegative\n",
        "#!zip -r /content/SplitPositive.zip /content/SplitPositive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PvOokaCPYRg1",
        "outputId": "0a76e08f-0f63-4c36-a10e-4a83144a2226"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_594c0a4d-78ed-4d7c-b024-ae85781929b7\", \"SplitNegative.zip\", 268831461)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(\"/content/SplitNegative.zip\")\n",
        "#files.download(\"/content/SplitPositive.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reviewing the files, I placed them in a google drive"
      ],
      "metadata": {
        "id": "Gflv71Ovl2kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ2OYyNfD0K1",
        "outputId": "8dc0b224-c176-4967-d838-56c197d6b836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n",
            "/content/gdrive/MyDrive/Rana Draytonii\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive\n",
        "%cd Rana\\ Draytonii # Location of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOeHSJ-ZEwsa",
        "outputId": "e676c519-3309-40b6-a139-5733bd5d75b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mRana1\u001b[0m/  \u001b[01;34mRana3\u001b[0m/  \u001b[01;34mRana4\u001b[0m/  \u001b[01;34mRana5\u001b[0m/  \u001b[01;34mResampledAudio\u001b[0m/  \u001b[01;34mSplitNegative\u001b[0m/  \u001b[01;34mSplitPositive\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gES2lpdAx9U5"
      },
      "source": [
        "**Resample to correct frequency, apply labels, and convert to tensors**\n",
        "\n",
        "\n",
        "\n",
        "**Before Running:**\n",
        "\n",
        "\n",
        "1.   specify path to positive and negative samples\n",
        "2.   create folder and specify path for resampled audio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn-H_dDc2yWL"
      },
      "source": [
        "**Prepare JSON files for datasets:**\n",
        "\n",
        "This script creates a JSON file for each of the train, validation, and test sets, each named train_data.json, val_data.json, and test_data.json, respectively.\n",
        "\n",
        "Each entry in the JSON file will contain the path to the audio file, and the corresponding label.\n",
        "\n",
        "Also, you might want to adjust the path of the wav field to match the environment where the model will be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Klxj4wZqDWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b908b08a-2a54-4672-8284-229c50a08ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key 'Positive' found in index_dict. Corresponding value is /m/positive\n",
            "Key 'Negative' found in index_dict. Corresponding value is /m/negative\n",
            "index_dict: {'Positive': '/m/positive', 'Negative': '/m/negative'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# Create labels.csv\n",
        "labels = {\n",
        "    'index': [0, 1],  # Modify the index values as per your labels\n",
        "    'mid': ['/m/positive', '/m/negative'],  # Modify the MID values as per your labels\n",
        "    'display_name': ['Positive', 'Negative']  # Modify the display names as per your labels\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(labels)\n",
        "df.to_csv('labels.csv', index=False)\n",
        "\n",
        "# Changes sampling frequency of audio file to 16kHz required by the AST model\n",
        "def resampler(audio_path, save_dir):\n",
        "    # load your audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # define resampler\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "\n",
        "    # resample the waveform\n",
        "    waveform_resampled = resampler(waveform)\n",
        "  \n",
        "    # create new filename\n",
        "    base_filename = os.path.basename(audio_path)\n",
        "    new_filename = os.path.splitext(base_filename)[0] + '_resampled.wav'\n",
        "    new_path = os.path.join(save_dir, new_filename)\n",
        "\n",
        "    # save the resampled audio\n",
        "    torchaudio.save(new_path, waveform_resampled, sample_rate=16000)\n",
        "\n",
        "    return new_path\n",
        "\n",
        "\n",
        "# Function to convert audio to Mel spectrogram (No longer used)\n",
        "def audio_to_mel_spectrogram(wav_name, mel_bins, target_length=1024):\n",
        "    waveform, sr = torchaudio.load(wav_name)\n",
        "    assert sr == 16000, 'input audio sampling rate must be 16kHz'\n",
        "\n",
        "    fbank = torchaudio.compliance.kaldi.fbank(\n",
        "        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
        "        window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10)\n",
        "\n",
        "    n_frames = fbank.shape[0]\n",
        "\n",
        "    p = target_length - n_frames\n",
        "    if p > 0:\n",
        "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "        fbank = m(fbank)\n",
        "    elif p < 0:\n",
        "        fbank = fbank[0:target_length, :]\n",
        "\n",
        "    fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
        "    return fbank\n",
        "\n",
        "# Function to create an index dictionary file per model specifications\n",
        "def make_index_dict(label_csv):\n",
        "    index_lookup = {}\n",
        "    with open(label_csv, 'r') as f:\n",
        "        csv_reader = csv.DictReader(f)\n",
        "        for row in csv_reader:\n",
        "            index_lookup[row['display_name']] = row['mid']\n",
        "    return index_lookup\n",
        "\n",
        "import json\n",
        "\n",
        "# Function to create .json file with filenames and labels per model specifications\n",
        "def create_data_json(dataset, labels, filename, index_dict):\n",
        "    data = []\n",
        "    for wav_path, label in zip(dataset, labels):\n",
        "        entry = {\n",
        "            \"wav\": wav_path,\n",
        "            \"labels\": index_dict[label],  \n",
        "        }\n",
        "        data.append(entry)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump({\"data\": data}, f, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "# Define the path where your positive and negative .wav files are stored\n",
        "positive_audio_path = 'SplitPositive'\n",
        "negative_audio_path = 'SplitNegative'\n",
        "# Define a directory to save resampled audio files (16kHz)\n",
        "resampled_audio_dir = 'ResampledAudio'\n",
        "\n",
        "# Define the target length for your spectrograms (only used for mel spectrogram)\n",
        "target_length = 1000\n",
        "mel_bins = 128  # Number of bins in Mel spectrogram\n",
        "\n",
        "# Define labels\n",
        "positive_label = 1\n",
        "negative_label = 0\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = []\n",
        "numeric_labels = []  # For train_test_split and torch.Tensor\n",
        "string_labels = []  # For JSON file\n",
        "\n",
        "# Process positive samples\n",
        "for filename in os.listdir(positive_audio_path):\n",
        "    if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "        filepath = os.path.join(positive_audio_path, filename)\n",
        "        filepath = resampler(filepath, resampled_audio_dir)  # Resample and get new file path\n",
        "        dataset.append(filepath)  # Save filepath instead of spectrogram\n",
        "        numeric_labels.append(positive_label)\n",
        "        string_labels.append('Positive')\n",
        "\n",
        "# Process negative samples\n",
        "for filename in os.listdir(negative_audio_path):\n",
        "    if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "        filepath = os.path.join(negative_audio_path, filename)\n",
        "        filepath = resampler(filepath, resampled_audio_dir)  # Resample and get new file path\n",
        "        dataset.append(filepath)  # Save filepath instead of spectrogram\n",
        "        numeric_labels.append(negative_label)\n",
        "        string_labels.append('Negative')\n",
        "\n",
        "numeric_labels = torch.Tensor(numeric_labels)\n",
        "\n",
        "# train_test_split\n",
        "dataset_trainval, dataset_test, labels_trainval, labels_test, string_labels_trainval, string_labels_test = train_test_split(dataset, numeric_labels, string_labels, test_size=0.15, random_state=42, stratify=numeric_labels)\n",
        "dataset_train, dataset_val, labels_train, labels_val, string_labels_train, string_labels_val = train_test_split(dataset_trainval, labels_trainval, string_labels_trainval, test_size=0.15, random_state=42, stratify=labels_trainval)\n",
        "\n",
        "index_dict = make_index_dict('labels.csv')\n",
        "\n",
        "# Check if the keys you'll use exist in the dictionary (testing)\n",
        "expected_keys = ['Positive', 'Negative']\n",
        "for key in expected_keys:\n",
        "    if key not in index_dict:\n",
        "        print(f\"Key '{key}' not found in index_dict\")\n",
        "    else:\n",
        "        print(f\"Key '{key}' found in index_dict. Corresponding value is {index_dict[key]}\")\n",
        "print(\"index_dict:\", index_dict)\n",
        "\n",
        "\n",
        "create_data_json(dataset_train, string_labels_train, 'train_data.json', index_dict)\n",
        "create_data_json(dataset_val, string_labels_val, 'val_data.json', index_dict)\n",
        "create_data_json(dataset_test, string_labels_test, 'test_data.json', index_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acRjFMUZKQtN"
      },
      "source": [
        "**Data Augmentation:**\n",
        "\n",
        "This will \"Stretch\" the data by applying augmentation that should change the audio files in ways that keep the integrity of the frog call but change it in ways that allows them to act as more new data to train on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "L7JSEVFLKXX8",
        "outputId": "3be8df79-1da8-49fb-fd61-f89dc6e3a364"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0c6c8a4f377f>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mwaveform_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mdataset_train_augmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform_augmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlabels_train_augmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The label remains the same after augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0c6c8a4f377f>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, waveform, sample_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Apply time shift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtime_shift_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_shift_seconds\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_shift_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Apply volume variation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchaudio.transforms' has no attribute 'TimeShift'"
          ]
        }
      ],
      "source": [
        "import torchaudio.transforms as T\n",
        "\n",
        "class AudioAugmentation:\n",
        "    def __init__(self, noise_std=0.01, time_shift_seconds=0.5, volume_var=0.1):\n",
        "        self.noise_std = noise_std\n",
        "        self.time_shift_seconds = time_shift_seconds\n",
        "        self.volume_var = volume_var\n",
        "\n",
        "    def __call__(self, waveform, sample_rate):\n",
        "        # Apply time shift\n",
        "        time_shift_samples = int(self.time_shift_seconds * sample_rate)\n",
        "        waveform = T.TimeShift(time_shift_samples)(waveform)\n",
        "\n",
        "        # Apply volume variation\n",
        "        volume_scale = 1 + self.volume_var * (torch.rand(1) - 0.5)\n",
        "        waveform = volume_scale * waveform\n",
        "\n",
        "        # Apply additive noise\n",
        "        noise = torch.normal(0, self.noise_std, waveform.shape)\n",
        "        waveform = waveform + noise\n",
        "        \n",
        "        return waveform.clamp_(-1, 1)  # Ensure waveform values are in [-1, 1]\n",
        "\n",
        "augment = AudioAugmentation()\n",
        "\n",
        "# Apply augmentation to each item in the training set\n",
        "dataset_train_augmented = []\n",
        "labels_train_augmented = []\n",
        "\n",
        "for (waveform, label) in zip(dataset_train, labels_train):\n",
        "    waveform_augmented = augment(waveform, 16000)\n",
        "    dataset_train_augmented.append(waveform_augmented)\n",
        "    labels_train_augmented.append(label)  # The label remains the same after augmentation\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "dataset_train_augmented = torch.stack(dataset_train_augmented)\n",
        "labels_train_augmented = torch.Tensor(labels_train_augmented)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}